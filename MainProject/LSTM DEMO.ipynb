{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaquin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(144, 1)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 144 entries, 0 to 143\n",
      "Data columns (total 1 columns):\n",
      "International airline passengers: monthly totals in thousands. Jan 49 ? Dec 60    144 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 1.2 KB\n",
      "None\n",
      "(144, 1)\n",
      "(96, 1)\n",
      "(48, 1)\n",
      "(94, 1, 1) (94,)\n",
      "(46, 1, 1) (46,)\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0414\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0202\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0145\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0131\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0121\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0111\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0102\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0093\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.0081\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0071\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.0062\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.0053\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.0045\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.0029\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.0026\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.0024\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 68/100\n"
     ]
    }
   ],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('..\\\\Datos\\\\international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "print(type(dataframe))\n",
    "print(dataframe.shape)\n",
    "print(dataframe.info())\n",
    "dataframe.head()\n",
    "\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "print(dataset.shape)\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "dataset.shape\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "print(trainX.shape,trainY.shape)\n",
    "print(testX.shape,testY.shape)\n",
    "\n",
    "# LSTM Network for Regression\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "print(trainPredict.shape)\n",
    "print(testPredict.shape)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "print(trainPredict.shape,trainY.shape)\n",
    "print(testPredict.shape,testY.shape)\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Regression Using the Window Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (1,) but got array with shape (94,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9ec66b86ab2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 955\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    956\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    790\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    134\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (1,) but got array with shape (94,)"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Regression with Time Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('..\\\\Datos\\\\international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 0.0287\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0117\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0097\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0086\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0074\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0065\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0058\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0053\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.0045\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0044\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.0041\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.0041\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.0040\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.0039\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.0039\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.0039\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.0039\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.0037\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.0037\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.0037\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.0037\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.0036\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.0037\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.0036\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.0036\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.0036\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.0036\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.0035\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.0036\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.0035\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.0035\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.0035\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.0036\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.0035\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.0035\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.0034\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.0034\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.0035\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.0034\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.0034\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.0034\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.0034\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.0032\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.0032\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.0032\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.0032\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.0032\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.0031\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.0029\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.0030\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.0030\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.0029\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.0030\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.0029\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.0029\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.0028\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.0029\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.0028\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.0028\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.0027\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.0026\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.0026\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.0026\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.0026\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.0025\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.0025\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.0024\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.0024\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.0024\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.0024\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.0024\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.0023\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.0021\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.0020\n",
      "Train Score: 23.70 RMSE\n",
      "Test Score: 58.77 RMSE\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0zStk/t/OMtfp4QBuvAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bf95fe8f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Memory Between Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('..\\\\Datos\\\\international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTMs with Memory Between Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('..\\\\Datos\\\\international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Univariate Time Series Forecasting\n",
    "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import to_datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    " \n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    " \n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    " \n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    " \n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    " \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = numpy.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    " \n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    " \n",
    "# load dataset\n",
    "series = read_csv('C:\\\\Users\\\\jquin\\\\Desktop\\\\alto_horizonte\\\\Datos\\\\shampoo-sales.csv',\n",
    "                  header=0, parse_dates=[0], squeeze=True,\n",
    "#                   ,index_col=0,\n",
    "#                   date_parser=parser\n",
    "                 )\n",
    "series = series.dropna()\n",
    "series.Month = [to_datetime('190'+x+'-01') for x in series.Month.values]\n",
    "series.set_index(series.Month,inplace=True)\n",
    "series = series.drop(['Month'],axis=1)\n",
    "\n",
    "# transform data to be stationary\n",
    "raw_values = series.values\n",
    "diff_values = difference(raw_values, 1)\n",
    " \n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    " \n",
    "# split data into train and test-sets\n",
    "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    " \n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)\n",
    " \n",
    "# fit the model\n",
    "lstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n",
    "# forecast the entire training dataset to build up state for forecasting\n",
    "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "lstm_model.predict(train_reshaped, batch_size=1)\n",
    " \n",
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "for i in range(len(test_scaled)):\n",
    "\t# make one-step forecast\n",
    "\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t# invert scaling\n",
    "\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t# invert differencing\n",
    "\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t# store forecast\n",
    "\tpredictions.append(yhat)\n",
    "\texpected = raw_values[len(train) + i + 1]\n",
    "\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    " \n",
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "pyplot.plot(raw_values[-12:])\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Multivariate Time Series Forecasting\n",
    "https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = numpy.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "# load dataset\n",
    "series = read_csv('C:\\\\Users\\\\jquin\\\\Desktop\\\\alto_horizonte\\\\Datos\\\\shampoo-sales.csv',\n",
    "                  header=0, parse_dates=[0], squeeze=True,\n",
    "#                   ,index_col=0,\n",
    "#                   date_parser=parser\n",
    "                 )\n",
    "series = series.dropna()\n",
    "series.Month = [to_datetime('190'+x+'-01') for x in series.Month.values]\n",
    "series.set_index(series.Month,inplace=True)\n",
    "series = series.drop(['Month'],axis=1)\n",
    "\n",
    "# transform data to be stationary\n",
    "raw_values = series.values\n",
    "diff_values = difference(raw_values, 1)\n",
    "\n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    "\n",
    "# split data into train and test-sets\n",
    "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    "\n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "# repeat experiment\n",
    "repeats = 30\n",
    "error_scores = list()\n",
    "for r in range(repeats):\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\ttrain_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "\tlstm_model.predict(train_reshaped, batch_size=1)\n",
    "\t# walk-forward validation on the test data\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions.append(yhat)\n",
    "\t# report performance\n",
    "\trmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "\tprint('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "\terror_scores.append(rmse)\n",
    "\n",
    "# summarize results\n",
    "results = DataFrame()\n",
    "results['rmse'] = error_scores\n",
    "print(results.describe())\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Multi-Step Time Series Forecasting\n",
    "https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "\t# extract raw values\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff_series = difference(raw_values, 1)\n",
    "\tdiff_values = diff_series.values\n",
    "\tdiff_values = diff_values.reshape(len(diff_values), 1)\n",
    "\t# rescale values to -1, 1\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaled_values = scaler.fit_transform(diff_values)\n",
    "\tscaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "\t# transform into supervised learning problem X, y\n",
    "\tsupervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "\tsupervised_values = supervised.values\n",
    "\t# split into train and test sets\n",
    "\ttrain, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "\treturn scaler, train, test\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "\t# reshape training into [samples, timesteps, features]\n",
    "\tX, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\t# design network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(y.shape[1]))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\t# fit network\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "\t# reshape input pattern to [samples, timesteps, features]\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\t# make forecast\n",
    "\tforecast = model.predict(X, batch_size=n_batch)\n",
    "\t# convert to array\n",
    "\treturn [x for x in forecast[0, :]]\n",
    "\n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "\tforecasts = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\tX, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "\t\t# make forecast\n",
    "\t\tforecast = forecast_lstm(model, X, n_batch)\n",
    "\t\t# store the forecast\n",
    "\t\tforecasts.append(forecast)\n",
    "\treturn forecasts\n",
    "\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "\t# invert first forecast\n",
    "\tinverted = list()\n",
    "\tinverted.append(forecast[0] + last_ob)\n",
    "\t# propagate difference forecast using inverted first value\n",
    "\tfor i in range(1, len(forecast)):\n",
    "\t\tinverted.append(forecast[i] + inverted[i-1])\n",
    "\treturn inverted\n",
    "\n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "\tinverted = list()\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\t# create array from forecast\n",
    "\t\tforecast = array(forecasts[i])\n",
    "\t\tforecast = forecast.reshape(1, len(forecast))\n",
    "\t\t# invert scaling\n",
    "\t\tinv_scale = scaler.inverse_transform(forecast)\n",
    "\t\tinv_scale = inv_scale[0, :]\n",
    "\t\t# invert differencing\n",
    "\t\tindex = len(series) - n_test + i - 1\n",
    "\t\tlast_ob = series.values[index]\n",
    "\t\tinv_diff = inverse_difference(last_ob, inv_scale)\n",
    "\t\t# store\n",
    "\t\tinverted.append(inv_diff)\n",
    "\treturn inverted\n",
    "\n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "\tfor i in range(n_seq):\n",
    "\t\tactual = [row[i] for row in test]\n",
    "\t\tpredicted = [forecast[i] for forecast in forecasts]\n",
    "\t\trmse = sqrt(mean_squared_error(actual, predicted))\n",
    "\t\tprint('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "\n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "\t# plot the entire dataset in blue\n",
    "\tpyplot.plot(series.values)\n",
    "\t# plot the forecasts in red\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\toff_s = len(series) - n_test + i - 1\n",
    "\t\toff_e = off_s + len(forecasts[i]) + 1\n",
    "\t\txaxis = [x for x in range(off_s, off_e)]\n",
    "\t\tyaxis = [series.values[off_s]] + forecasts[i]\n",
    "\t\tpyplot.plot(xaxis, yaxis, color='red')\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\n",
    "# load dataset\n",
    "series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "# configure\n",
    "n_lag = 1\n",
    "n_seq = 3\n",
    "n_test = 10\n",
    "n_epochs = 1500\n",
    "n_batch = 1\n",
    "n_neurons = 1\n",
    "# prepare data\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "# fit model\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "# make forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "actual = [row[n_lag:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "# evaluate forecasts\n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "# plot forecasts\n",
    "plot_forecasts(series, forecasts, n_test+2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
